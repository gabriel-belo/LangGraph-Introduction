{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n",
    "\n",
    "# State \n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages, config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State)-> Literal [\"summarize_conversation\",END]:\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59341b",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Agora, vamos falar sobre maneiras de transmitir o estado do nosso grafo.\n",
    "\n",
    "`.stream` e `.astream` são métodos síncronos e assíncronos para transmitir resultados.\n",
    "\n",
    "O LangGraph suporta alguns modos diferentes de transmissão do estado do grafo.\n",
    "\n",
    "* `values`: Transmite o estado completo do grafo após cada chamada de nó.\n",
    "\n",
    "* `updates`: Transmite as atualizações do estado do grafo após cada chamada de nó."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb5831",
   "metadata": {},
   "source": [
    "Vamos testar primeiramente stream_mode= \"updates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar thread\n",
    "config= {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Iniciar conversa\n",
    "for chunck in graph.stream({\"messages\": [HumanMessage(content= \"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311313fb",
   "metadata": {},
   "source": [
    "Printando só o state update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8adf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunck in graph.stream({\"messages\": [HumanMessage(content= \"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    chunck['conversation']['messages'].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1bfacf",
   "metadata": {},
   "source": [
    "Agora utilizando o stream_mode= \"values\"\n",
    "\n",
    "Este é o state completo do graph depois do nó conversation ser chamado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45be731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando thread\n",
    "config= {\"configuration\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "input_message= HumanMessage(content= \"hi! I'm Lance\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    for m in event[\"messages\"]:\n",
    "        m.pretty_print()\n",
    "    print(\"----\" * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113308e4",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "Frequentemente, queremos transmitir mais do que apenas o estado do grafo.\n",
    "\n",
    "Em particular, com chamadas do modelo de chat, é comum transmitir os tokens à medida que são gerados.\n",
    "\n",
    "Podemos fazer isso usando o método `.astream_events`, que transmite eventos conforme eles ocorrem dentro dos nós!\n",
    "\n",
    "Cada evento é um dicionário com algumas chaves:\n",
    "\n",
    "* `event`: Este é o tipo de evento que está sendo emitido.\n",
    "\n",
    "* `name`: Este é o nome do evento.\n",
    "\n",
    "* `data`: Estes são os dados associados ao evento.\n",
    "\n",
    "* `metadata`: Contém `langgraph_node`, o nó que emitiu o evento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110aa567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de1f96",
   "metadata": {},
   "source": [
    "O ponto central é que os tokens dos modelos de chat dentro do seu grafo têm o tipo `on_chat_model_stream`.\n",
    "\n",
    "Podemos usar `event['metadata']['langgraph_node']` para selecionar o nó do qual transmitir os dados.\n",
    "\n",
    "E podemos usar `event['data']` para obter os dados reais de cada evento, que neste caso é um `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a68b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da90263",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()\n",
    "\n",
    "\n",
    "#Os objetos transmitidos possuem:\n",
    "#evento: Tipo\n",
    "#dados: Estado\n",
    "\n",
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965e431",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
