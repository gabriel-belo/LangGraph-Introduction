{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54bee17d",
   "metadata": {},
   "source": [
    "## Chatbot com resumo de mensagens\n",
    "\n",
    "LLMs para gerar um resumo contínuo da conversa.\n",
    "\n",
    "Isso nos permite manter uma representação compacta da conversa completa, em vez de simplesmente removê-la com cortes ou filtros.\n",
    "\n",
    "Incorporaremos esse resumo em um chatbot simples.\n",
    "\n",
    "E equiparemos esse chatbot com memória, permitindo conversas de longa duração sem incorrer em alto custo de tokens/latência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7169a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89474f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "class State(MessagesState):\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ea0efb",
   "metadata": {},
   "source": [
    "### O que é state.get(\"summary\", \"\")?\n",
    "\n",
    "* O método .get() tenta recuperar o valor associado à chave \"summary\" dentro do objeto state.\n",
    "\n",
    "* Se a chave \"summary\" existir, seu valor é atribuído à variável summary.\n",
    "\n",
    "* Se a chave \"summary\" não existir (ou for None), o método .get() retorna o valor padrão fornecido como segundo argumento, que é uma string vazia (\"\").\n",
    "\n",
    "* Propósito: Garante que a variável summary sempre terá um valor (ou o resumo existente, ou uma string vazia), evitando erros se o campo de resumo não tiver sido preenchido no estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "\n",
    "def call_model(state: State):\n",
    "\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    if summary:\n",
    "\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        messages= [SystemMessage(content= system_message)] + state[\"messages\"]\n",
    "\n",
    "    else:\n",
    "        messages= state[\"messages\"]\n",
    "\n",
    "    response= model.invoke(messages)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Crie nosso prompt de resumo\n",
    "    if summary:\n",
    "        \n",
    "        # Já existe um resumo\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Adicionar prompt ao nosso histórico\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    \n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from typing_extensions import Literal\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State) -> Literal [\"summarize_conversation\", END]:\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144f318",
   "metadata": {},
   "source": [
    "## Adicionando memória\n",
    "Lembre-se de que o estado é transitório para uma única execução do grafo.\n",
    "\n",
    "Isso limita nossa capacidade de ter conversas com múltiplas etapas e interrupções.\n",
    "\n",
    "Como apresentado no final do Módulo 1, podemos usar persistência para resolver isso!\n",
    "\n",
    "O LangGraph pode usar um checkpoint para salvar automaticamente o estado do grafo após cada etapa.\n",
    "\n",
    "Essa camada de persistência integrada fornece memória, permitindo que o LangGraph retome a execução a partir da última atualização de estado.\n",
    "\n",
    "Como mostramos anteriormente, uma das opções mais fáceis de usar é o MemorySaver, um armazenamento de chave-valor em memória para o estado do grafo.\n",
    "\n",
    "Tudo o que precisamos fazer é compilar o grafo com um checkpoint e nosso grafo terá memória!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860302a",
   "metadata": {},
   "source": [
    "## Threads\n",
    "\n",
    "O mecanismo de checkpoint salva o estado a cada passo como um ponto de verificação.\n",
    "\n",
    "Esses pontos de verificação salvos podem ser agrupados em uma sequência de conversa.\n",
    "\n",
    "Pense no Slack como uma analogia: diferentes canais carregam diferentes conversas.\n",
    "\n",
    "As sequências são como canais do Slack, capturando conjuntos agrupados de estado (por exemplo, conversas).\n",
    "\n",
    "Abaixo, usamos um recurso configurável para definir um ID de sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criando uma thread\n",
    "config= {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Iniciando a conversa\n",
    "input_message= HumanMessage(content= \"Hi! I'm Lance\")\n",
    "output= graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output[\"messages\"][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message= HumanMessage(content= \"what's my name?\")\n",
    "output= graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output[\"messages\"][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message= HumanMessage(content= \"I like the 49ers!\")\n",
    "output= graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output[\"messages\"][-1:]:\n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a81e4",
   "metadata": {},
   "source": [
    "Como definimos para possuir um resumo somente após possuir mais de 6 mensagens, nós ainda não temos um resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(config).values.get(\"summary\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f199035",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
